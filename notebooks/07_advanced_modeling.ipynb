{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Modeling\n",
    "\n",
    "# We will build an advanced recommendation model using the getINNOtized dataset. This involves:\n",
    "# - Loading the preprocessed user-item interaction matrix with anomaly filtering (users with >100 events/day removed).\n",
    "# - Training an SVD model with tuned parameters for collaborative filtering.\n",
    "# - Evaluating the model using RMSE and Precision@5.\n",
    "\n",
    "# This model will address business questions:\n",
    "# - Q1: Personalization (collaborative filtering).\n",
    "# - Q5: Anomaly filtering (removing bots/outliers).\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactions_df shape: (811499, 3)\n",
      "Rating distribution:\n",
      " rating\n",
      "1    719082\n",
      "2     48417\n",
      "5     18494\n",
      "3     17191\n",
      "4      8315\n",
      "Name: count, dtype: int64\n",
      "Optimized RMSE: 0.7176\n",
      "Sampled Precision@5: 0.0110\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "import joblib\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Optimized Data Loading ---\n",
    "preprocessed_data_dir = \"../data/preprocessed_data/\"\n",
    "\n",
    "# Vectorized mapping loader with error handling\n",
    "def load_mapping(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, header=None, skiprows=1, names=['key', 'value'])\n",
    "        return df.set_index('key')['value'].to_dict()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Load mappings\n",
    "user_ids = load_mapping(os.path.join(preprocessed_data_dir, \"user_ids.csv\"))\n",
    "item_ids = load_mapping(os.path.join(preprocessed_data_dir, \"item_ids.csv\"))\n",
    "\n",
    "# Reverse mappings\n",
    "user_idx_to_id = {v: k for k, v in user_ids.items()}\n",
    "item_idx_to_id = {v: k for k, v in item_ids.items()}\n",
    "\n",
    "# --- Sparse Matrix Optimization ---\n",
    "with open(os.path.join(preprocessed_data_dir, \"user_item_sparse.pkl\"), \"rb\") as f:\n",
    "    user_item_matrix = joblib.load(f)\n",
    "\n",
    "# Convert to CSR for optimization, then to COO for DataFrame conversion\n",
    "user_item_csr = user_item_matrix.tocsr()\n",
    "user_item_coo = user_item_csr.tocoo()\n",
    "\n",
    "# --- Precompute Category Mapping ---\n",
    "item_features = pd.read_csv(os.path.join(preprocessed_data_dir, \"item_features.csv\"))\n",
    "category_to_items = item_features.groupby('categoryid')['item_idx'].agg(list).to_dict()\n",
    "item_to_category = item_features.set_index('item_idx')['categoryid'].to_dict()\n",
    "\n",
    "# --- Convert sparse matrix to DataFrame ---\n",
    "interactions_df = pd.DataFrame(\n",
    "    [(user_idx_to_id[row], item_idx_to_id[col], rating) for row, col, rating in zip(user_item_coo.row, user_item_coo.col, user_item_coo.data)],\n",
    "    columns=['user_id', 'item_id', 'rating']\n",
    ")\n",
    "\n",
    "print(\"Interactions_df shape:\", interactions_df.shape)\n",
    "print(\"Rating distribution:\\n\", interactions_df['rating'].value_counts())\n",
    "\n",
    "# --- Model Optimization ---\n",
    "svd = SVD(n_factors=20, n_epochs=15, lr_all=0.01, random_state=42)\n",
    "\n",
    "# Load full dataset into Surprise\n",
    "data = Dataset.load_from_df(interactions_df[['user_id', 'item_id', 'rating']], Reader(rating_scale=(1, 5)))\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit model and compute predictions\n",
    "svd.fit(trainset)\n",
    "svd_predictions = svd.test(testset)\n",
    "\n",
    "# --- Optimized Recommendation Functions ---\n",
    "def get_similar_items(item_id, top_n=5):\n",
    "    item_idx = item_ids.get(item_id)\n",
    "    if item_idx is None or item_idx not in item_to_category:\n",
    "        return interactions_df['item_id'].sample(min(top_n, len(interactions_df))).tolist()\n",
    "    \n",
    "    category = item_to_category[item_idx]\n",
    "    similar_items = category_to_items.get(category, [])[:top_n * 2]\n",
    "    return [item_idx_to_id[item] for item in similar_items if item != item_idx][:top_n]\n",
    "\n",
    "def hybrid_recommendation(user_id, top_n=5, alpha=0.7):\n",
    "    user_preds = [(pred.iid, pred.est) for pred in svd_predictions if pred.uid == user_id]\n",
    "    svd_recs = [iid for iid, _ in sorted(user_preds, key=lambda x: x[1], reverse=True)][:top_n] if user_preds else []\n",
    "    \n",
    "    if not svd_recs:\n",
    "        user_ratings = interactions_df[interactions_df['user_id'] == user_id]\n",
    "        svd_recs = user_ratings['item_id'].sample(min(top_n, len(user_ratings))).tolist() if not user_ratings.empty else \\\n",
    "                   interactions_df['item_id'].sample(top_n).tolist()\n",
    "    \n",
    "    cb_recs = get_similar_items(svd_recs[0], top_n) if svd_recs else interactions_df['item_id'].sample(top_n).tolist()\n",
    "    combined = pd.Series(svd_recs + cb_recs).value_counts().index.tolist()\n",
    "    return combined[:top_n]\n",
    "\n",
    "# --- Optimized Evaluation ---\n",
    "def precision_at_k_hybrid(test_df, k=5, threshold=3):\n",
    "    test_df = pd.DataFrame(testset, columns=['user_id', 'item_id', 'rating'])\n",
    "    sampled_users = test_df['user_id'].drop_duplicates().sample(frac=0.2, random_state=42)\n",
    "    \n",
    "    precisions = []\n",
    "    for uid in sampled_users:\n",
    "        recs = hybrid_recommendation(uid, k)\n",
    "        user_ratings = test_df[(test_df['user_id'] == uid) & (test_df['rating'] >= threshold)]\n",
    "        relevant_items = set(user_ratings['item_id'])\n",
    "        hits = len(set(recs) & relevant_items)\n",
    "        precisions.append(hits / k if k > 0 else 0)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "# --- Execute ---\n",
    "if __name__ == \"__main__\":\n",
    "    svd.fit(trainset)  # Ensure model is fitted\n",
    "    rmse = accuracy.rmse(svd_predictions, verbose=False)\n",
    "    precision = precision_at_k_hybrid(pd.DataFrame(testset, columns=['user_id', 'item_id', 'rating']), k=5, threshold=3)\n",
    "    \n",
    "    print(f\"Optimized RMSE: {rmse:.4f}\")\n",
    "    print(f\"Sampled Precision@5: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 11:36:34,294] A new study created in memory with name: no-name-72795ab1-82d0-42c7-938b-137fc98ead13\n",
      "[I 2025-03-13 11:36:34,370] Trial 0 pruned. Memory limit exceeded\n",
      "[I 2025-03-13 11:36:34,387] Trial 1 pruned. Memory limit exceeded\n",
      "[I 2025-03-13 11:36:34,414] Trial 2 pruned. Memory limit exceeded\n",
      "[I 2025-03-13 11:36:34,429] Trial 3 pruned. Memory limit exceeded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 11:36:34,443] Trial 4 pruned. Memory limit exceeded\n",
      "[I 2025-03-13 11:36:34,458] Trial 5 pruned. Memory limit exceeded\n",
      "[I 2025-03-13 11:36:34,474] Trial 6 pruned. Memory limit exceeded\n",
      "[I 2025-03-13 11:36:34,499] Trial 7 pruned. Memory limit exceeded\n",
      "[I 2025-03-13 11:36:34,515] Trial 8 pruned. Memory limit exceeded\n",
      "[I 2025-03-13 11:36:34,643] Trial 9 pruned. Memory limit exceeded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No trials are completed yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 152\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    150\u001b[0m     psutil \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal execution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 122\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Train final model\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining final model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 122\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params\u001b[49m\n\u001b[0;32m    123\u001b[0m recommender \u001b[38;5;241m=\u001b[39m OptimizedHybridRecommender(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_params)\n\u001b[0;32m    124\u001b[0m recommender\u001b[38;5;241m.\u001b[39mfit(train, item_features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\optuna\\study\\study.py:119\u001b[0m, in \u001b[0;36mStudy.best_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbest_params\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return parameters of the best trial in the study.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    116\u001b[0m \n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_trial\u001b[49m\u001b[38;5;241m.\u001b[39mparams\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\optuna\\study\\study.py:162\u001b[0m, in \u001b[0;36mStudy.best_trial\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_multi_objective():\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA single best trial cannot be retrieved from a multi-objective study. Consider \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing Study.best_trials to retrieve a list containing the best trials.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 162\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_study_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# If the trial with the best value is infeasible, select the best trial from all feasible\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# trials. Note that the behavior is undefined when constrained optimization without the\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# violation value in the best-valued trial.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m constraints \u001b[38;5;241m=\u001b[39m best_trial\u001b[38;5;241m.\u001b[39msystem_attrs\u001b[38;5;241m.\u001b[39mget(_CONSTRAINTS_KEY)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\optuna\\storages\\_in_memory.py:249\u001b[0m, in \u001b[0;36mInMemoryStorage.get_best_trial\u001b[1;34m(self, study_id)\u001b[0m\n\u001b[0;32m    246\u001b[0m best_trial_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_studies[study_id]\u001b[38;5;241m.\u001b[39mbest_trial_id\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_trial_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo trials are completed yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_studies[study_id]\u001b[38;5;241m.\u001b[39mdirections) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial can be obtained only for single-objective optimization.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: No trials are completed yet."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "import implicit\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "from implicit.evaluation import precision_at_k\n",
    "import joblib\n",
    "from numba import jit\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "\n",
    "# Configuration (Optimized for 12GB RAM)\n",
    "DATA_DIR = \"../data/preprocessed_data/\"\n",
    "MODEL_DIR = \"../models/\"\n",
    "RESULTS_DIR = \"../results/\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Hardware-Aware Settings\n",
    "MAX_THREADS = 4\n",
    "BATCH_SIZE = 512\n",
    "MAX_MEMORY_USAGE = 10_000_000_000\n",
    "\n",
    "def load_sparse_matrix():\n",
    "    \"\"\"Memory-mapped sparse matrix loading\"\"\"\n",
    "    matrix_path = os.path.join(DATA_DIR, \"user_item_sparse.pkl\")\n",
    "    if os.path.exists(matrix_path):\n",
    "        matrix = joblib.load(matrix_path)\n",
    "        return matrix.tocsr().astype(np.float32)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Sparse matrix not found at {matrix_path}\")\n",
    "\n",
    "class OptimizedHybridRecommender:\n",
    "    def __init__(self, n_factors=80, iterations=12, regularization=0.1, \n",
    "                 alpha=0.8, k=5, diversity_weight=0.2):\n",
    "        self.model = implicit.als.AlternatingLeastSquares(\n",
    "            factors=n_factors,\n",
    "            iterations=iterations,\n",
    "            regularization=regularization,\n",
    "            random_state=42,\n",
    "            use_gpu=False,\n",
    "            num_threads=MAX_THREADS\n",
    "        )\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.diversity_weight = diversity_weight\n",
    "        self.item_features = None\n",
    "        self.user_item_matrix = None\n",
    "        self.popular_items = None\n",
    "\n",
    "    def fit(self, user_item_matrix, item_features):\n",
    "        weighted_matrix = self._batch_weight(user_item_matrix)\n",
    "        self.model.fit(weighted_matrix.T * self.alpha)\n",
    "        self.item_features = item_features.astype(np.float32)\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        self._calculate_popularity()\n",
    "\n",
    "    def _batch_weight(self, matrix):\n",
    "        batch_size = BATCH_SIZE\n",
    "        n_users = matrix.shape[0]\n",
    "        weighted_data = []\n",
    "        for start in tqdm(range(0, n_users, batch_size), desc=\"Batch Weighting\"):\n",
    "            end = min(start + batch_size, n_users)\n",
    "            batch = matrix[start:end].copy()\n",
    "            weighted_batch = bm25_weight(batch, K1=100, B=0.8)\n",
    "            weighted_data.append(weighted_batch)\n",
    "        return csr_matrix(np.vstack(weighted_data)) if weighted_data else csr_matrix(matrix.shape)\n",
    "\n",
    "    def _calculate_popularity(self):\n",
    "        item_counts = np.zeros(self.user_item_matrix.shape[1], dtype=np.float32)\n",
    "        for i in tqdm(range(self.user_item_matrix.shape[1]), desc=\"Popularity Calc\"):\n",
    "            item_counts[i] = self.user_item_matrix[:,i].sum()\n",
    "        self.popular_items = np.argpartition(-item_counts, self.k*2)[:self.k*2]\n",
    "\n",
    "    def recommend(self, user_id, user_items):\n",
    "        ids, _ = self.model.recommend(\n",
    "            user_id, \n",
    "            user_items,\n",
    "            N=self.k,\n",
    "            filter_already_liked=True,\n",
    "            recalculate_user=False\n",
    "        )\n",
    "        if len(ids) < self.k:\n",
    "            needed = self.k - len(ids)\n",
    "            ids = np.concatenate([ids, self.popular_items[:needed]])\n",
    "        return ids[:self.k]\n",
    "\n",
    "def optimize_hyperparameters(trial):\n",
    "    return {\n",
    "        'n_factors': trial.suggest_int('n_factors', 64, 128),\n",
    "        'iterations': trial.suggest_int('iterations', 8, 15),\n",
    "        'regularization': trial.suggest_float('regularization', 0.05, 0.3),\n",
    "        'alpha': trial.suggest_float('alpha', 0.5, 1.2),\n",
    "        'diversity_weight': trial.suggest_float('diversity_weight', 0.1, 0.3)\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    start_mem = psutil.virtual_memory().used if psutil else 0\n",
    "    print(\"Loading data...\")\n",
    "    user_item_matrix = load_sparse_matrix()\n",
    "    # Load item features from CSV instead of .pkl\n",
    "    item_features_df = pd.read_csv(os.path.join(DATA_DIR, \"item_features.csv\"))\n",
    "    # Convert item_features_df to a sparse matrix if needed (e.g., one-hot encode categories)\n",
    "    item_features = pd.get_dummies(item_features_df['categoryid']).astype(np.float32).values\n",
    "    \n",
    "    # Train-test split\n",
    "    train, test = implicit.evaluation.train_test_split(\n",
    "        user_item_matrix, \n",
    "        train_percentage=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Hyperparameter optimization\n",
    "    print(\"Optimizing hyperparameters...\")\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, train, test), n_trials=10, timeout=1200)\n",
    "    \n",
    "    # Train final model\n",
    "    print(\"Training final model...\")\n",
    "    best_params = study.best_params\n",
    "    recommender = OptimizedHybridRecommender(**best_params)\n",
    "    recommender.fit(train, item_features)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"Evaluating model...\")\n",
    "    precision = precision_at_k(recommender.model, train, test, K=5, num_threads=MAX_THREADS)\n",
    "    \n",
    "    print(f\"\\nFinal Metrics:\")\n",
    "    print(f\"Precision@5: {precision:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(recommender, os.path.join(MODEL_DIR, \"optimized_recommender.pkl\"), compress=3)\n",
    "\n",
    "def objective(trial, train, test):\n",
    "    if psutil and (psutil.virtual_memory().available < MAX_MEMORY_USAGE):\n",
    "        raise optuna.TrialPruned(\"Memory limit exceeded\")\n",
    "    \n",
    "    params = optimize_hyperparameters(trial)\n",
    "    model = OptimizedHybridRecommender(**params)\n",
    "    model.fit(train, item_features)\n",
    "    return precision_at_k(model.model, train, test, K=5, num_threads=MAX_THREADS)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        import psutil\n",
    "    except ImportError:\n",
    "        psutil = None\n",
    "    \n",
    "    main()\n",
    "    print(f\"\\nTotal execution time: {(time.time() - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "# We have trained an SVD model with tuned parameters on the corrected user-item interaction matrix.\n",
    "# - RMSE and Precision@5 indicate the model's performance.\n",
    "# - This model addresses Q1 (personalization) and Q5 (anomaly filtering).\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "# - Address Q4 (popular products): Analyze item popularity.\n",
    "# - Address Q6 (diversity): Incorporate item and category features for a hybrid model.\n",
    "# - Address Q7 (algorithm comparison): Compare SVD with other algorithms (e.g., KNN, NMF)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
