{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Baseline Model\n",
    "\n",
    "I will build a baseline recommendation model using collaborative filtering with Singular Value Decomposition (SVD). I will use the preprocessed user-item interaction matrix from `04_feature_engineering.ipynb` to:\n",
    "- Train an SVD model.\n",
    "- Evaluate performance using RMSE and Precision@K.\n",
    "- Address business questions like personalization, conversion optimization, and algorithm performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model for Product Recommendations\n",
    "\n",
    "## Introduction\n",
    "This notebook aims to build a baseline model for product recommendations based on user interactions and features derived from the data. The model will address the following business questions:\n",
    "\n",
    "1. How can we personalize product recommendations to enhance user satisfaction and retention?\n",
    "2. What are the most effective strategies to improve conversion rates from views to transactions?\n",
    "3. How do seasonal patterns or trends influence user interactions and sales?\n",
    "4. Which products or categories are most popular or have the highest interaction rates?\n",
    "5. How can we identify and filter out bot activity or anomalous user behavior to ensure accurate recommendations?\n",
    "6. How can we balance recommendation accuracy with diversity to cater to a wide range of user preferences?\n",
    "7. What machine learning algorithm provides the most accurate and efficient recommendations for our platform?\n",
    "\n",
    "## Data Loading\n",
    "We will load the preprocessed data, which includes user-item interactions and product information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Summary\n",
    "\n",
    "I will develop and optimize the following components for the baseline recommendation model:\n",
    "- **SVD Model**: A collaborative filtering model using Singular Value Decomposition (SVD) with 20 factors, trained on the user-item interaction data, saved as part of the notebook execution output.\n",
    "- **Performance Metrics**: Computed Root Mean Squared Error (RMSE) and Precision@5 metrics to evaluate model accuracy and relevance, displayed as output (RMSE: 1.5190, Precision@5: 0.0027).\n",
    "- **Interaction DataFrame**: A processed DataFrame (`interactions_df`) mapping user-item interactions back to original IDs, generated during execution for use in model training.\n",
    "\n",
    "These components will establish a baseline for personalized recommendations, partially addressing business questions related to personalization and algorithm performance, and serve as a foundation for further modeling to tackle all seven business questions and project objectives.\n",
    "\n",
    "### Details\n",
    "- **Input Data**: Utilizes preprocessed files `user_item_sparse.pkl`, `user_ids.csv`, and `item_ids.csv` from feature engineering.\n",
    "- **Optimizations**: Reduced test set size to 10% and SVD factors to 20 for faster runtime (<30 minutes).\n",
    "- **Output**: Model performance metrics (RMSE, Precision@5) printed to console, with the trained model ready for evaluation and refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.5190\n",
      "RMSE on Test Set: 1.5190\n",
      "Precision@5: 0.0027\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "preprocessed_data_dir = \"../data/preprocessed_data/\"\n",
    "\n",
    "# Load user and item ID mappings\n",
    "try:\n",
    "    user_ids = pd.read_csv(os.path.join(preprocessed_data_dir, \"user_ids.csv\"), index_col=\"Unnamed: 0\")[\"0\"].to_dict()\n",
    "    item_ids = pd.read_csv(os.path.join(preprocessed_data_dir, \"item_ids.csv\"), index_col=\"Unnamed: 0\")[\"0\"].to_dict()\n",
    "    # Create inverse mappings for faster lookups\n",
    "    user_idx_to_id = {v: k for k, v in user_ids.items()}\n",
    "    item_idx_to_id = {v: k for k, v in item_ids.items()}\n",
    "except KeyError as e:\n",
    "    print(f\"Error loading ID mappings: {e}. Check CSV structure.\")\n",
    "    raise\n",
    "\n",
    "# Load the sparse matrix using joblib\n",
    "try:\n",
    "    with open(os.path.join(preprocessed_data_dir, \"user_item_sparse.pkl\"), \"rb\") as f:\n",
    "        user_item_matrix = joblib.load(f)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading sparse matrix: {e}. Ensure itâ€™s regenerated with current NumPy.\")\n",
    "    raise\n",
    "\n",
    "# Convert sparse matrix to DataFrame for surprise (optimized)\n",
    "user_item_coo = coo_matrix(user_item_matrix)\n",
    "rows, cols, data = user_item_coo.row, user_item_coo.col, user_item_coo.data\n",
    "\n",
    "# Create interactions list with optimized mapping\n",
    "interactions_list = [(user_idx_to_id[row], item_idx_to_id[col], rating) for row, col, rating in zip(rows, cols, data)]\n",
    "\n",
    "# Create a DataFrame\n",
    "interactions_df = pd.DataFrame(interactions_list, columns=['user_id', 'item_id', 'rating'])\n",
    "\n",
    "# Define the reader for surprise\n",
    "reader = Reader(rating_scale=(1, 5))  # Matches event weights: view=1, addtocart=3, transaction=5\n",
    "\n",
    "# Load data into surprise Dataset\n",
    "data = Dataset.load_from_df(interactions_df[['user_id', 'item_id', 'rating']], reader)\n",
    "\n",
    "# Split into train and test sets (use a smaller test size for speed)\n",
    "trainset, testset = train_test_split(data, test_size=0.1, random_state=42)  # Reduced to 10% for faster testing\n",
    "\n",
    "# Train SVD model with fewer factors for speed\n",
    "svd = SVD(n_factors=20, random_state=42)  # Reduced from 50 to 20\n",
    "svd.fit(trainset)\n",
    "\n",
    "# Evaluate on test set\n",
    "predictions = svd.test(testset)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = accuracy.rmse(predictions)\n",
    "print(f\"RMSE on Test Set: {rmse:.4f}\")\n",
    "\n",
    "# Simplified Precision@K (K=5) for quick results\n",
    "from collections import defaultdict\n",
    "\n",
    "def precision_at_k(predictions, k=5, threshold=3):\n",
    "    \"\"\"Return average precision at k for quick evaluation.\"\"\"\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = []\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        n_rel_and_rec_k = sum((true_r >= threshold and est >= threshold)\n",
    "                            for est, true_r in user_ratings[:k])\n",
    "        n_rec_k = min(k, len(user_ratings))\n",
    "        precisions.append(n_rel_and_rec_k / n_rec_k if n_rec_k > 0 else 0)\n",
    "\n",
    "    return np.mean(precisions) if precisions else 0\n",
    "\n",
    "precision_at_k = precision_at_k(predictions, k=5, threshold=3)\n",
    "print(f\"Precision@5: {precision_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Questions Addressed\n",
    "- **Q1 (Personalize recommendations)**: Yes, via SVD predictions.\n",
    "- **Q2 (Improve conversion rates)**: Partially, via Precision@5, but needs further optimization.\n",
    "- **Q7 (Best algorithm)**: Partially, as a baseline for comparison.\n",
    "\n",
    "### Remaining Business Questions\n",
    "- **Q3 (Seasonal patterns)**: Requires time-based features.\n",
    "- **Q4 (Popular products)**: Needs post-analysis.\n",
    "- **Q5 (Anomaly filtering)**: Requires pre-processing.\n",
    "- **Q6 (Diversity)**: Needs hybrid modeling.\n",
    "\n",
    "### Project Objectives Supported\n",
    "- **Personalization**: Achieved with SVD.\n",
    "- **Conversion Rates**: Partially achieved.\n",
    "- **Historical Data Utilization**: Achieved.\n",
    "- **Engagement**: Partially achieved.\n",
    "- **Scalability**: Achieved with optimizations.\n",
    "- **Sales Boost**: Partially achieved.\n",
    "- **Diversity**: Not achieved.\n",
    "\n",
    "### Next Steps\n",
    "- **Model Tuning**: Adjust parameters (e.g., factors, threshold) using cross-validation.\n",
    "- **Advanced Modeling**: Incorporate time features, anomaly filtering, and hybrid models for diversity.\n",
    "- **Evaluation**: Compare with other algorithms and validate in a live setting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
