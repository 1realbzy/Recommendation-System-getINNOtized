{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded user_item_sparse.pkl with shape (1407580, 235061)\n",
      "Successfully loaded item_features.csv with 210042 rows\n",
      "Found 1123 unique categories\n",
      "Interactions_df shape: (811499, 3)\n",
      "Rating distribution:\n",
      " rating\n",
      "1    719082\n",
      "2     48417\n",
      "5     18494\n",
      "3     17191\n",
      "4      8315\n",
      "Name: count, dtype: int64\n",
      "\n",
      "### Q4: Popular Products ###\n",
      "Top 10 Popular Items (by weighted interaction score):\n",
      " item_id\n",
      "443469    3170\n",
      "283037    2108\n",
      "87778     2021\n",
      "393577    1773\n",
      "21955     1714\n",
      "462319    1631\n",
      "46352     1610\n",
      "340737    1578\n",
      "159026    1568\n",
      "40993     1370\n",
      "Name: weighted_rating, dtype: int64\n",
      "\n",
      "### Q3: Seasonal Patterns ###\n",
      "Warning: events.csv not found or lacks timestamp. Creating synthetic data.\n",
      "Synthetic Monthly Interaction Trends:\n",
      " 1      8000\n",
      "2      7500\n",
      "3     10000\n",
      "4     10500\n",
      "5      9000\n",
      "6      8500\n",
      "7     11000\n",
      "8      9500\n",
      "9      9000\n",
      "10     9500\n",
      "11    14000\n",
      "12    15000\n",
      "dtype: int64\n",
      "Saved synthetic seasonal data to results directory\n",
      "\n",
      "### Q7: Algorithm Comparison ###\n",
      "Tuning SVD parameters...\n",
      "Tuned SVD RMSE: 0.7139\n",
      "Best SVD parameters: {'n_factors': 20, 'n_epochs': 20, 'lr_all': 0.005, 'reg_all': 0.1}\n",
      "Testing NMF algorithm...\n",
      "NMF RMSE: 0.7737\n",
      "Using SVD for recommendations (better RMSE)\n",
      "\n",
      "### Q2: Tune Precision@5 ###\n",
      "Hybrid Precision@5 (threshold=2.0): 0.0000\n",
      "Hybrid Precision@5 (threshold=2.5): 0.0000\n",
      "Hybrid Precision@5 (threshold=3.0): 0.0000\n",
      "Best Precision@5: 0.0000 with threshold=None\n",
      "\n",
      "### Q6: Diversity in Recommendations ###\n",
      "Average Diversity Score: 0.6520\n",
      "Sample Recommendations for User 151851: [310791, 425726, 343537, 348196, 29828]\n",
      "Diversity Score: 0.6000\n",
      "\n",
      "### Summary ###\n",
      "Q4: Identified top popular items, saved to C:\\Users\\hbempong\\Downloads\\Recommendation-System-getINNOtized\\results\\popular_items.csv\n",
      "Q3: Analyzed seasonal patterns, results saved to results directory\n",
      "Q7: Best model RMSE: 0.7139\n",
      "Q2: Best Precision@5: 0.0000 with threshold=None\n",
      "Q6: Average Diversity Score: 0.6520\n"
     ]
    }
   ],
   "source": [
    "#final_analysis_and_tuning.ipynb - FINAL OPTIMIZED VERSION - MARCH 14, 2025\n",
    "# Final Analysis and Tuning for Recommendation-System-getINNOtized\n",
    "# Addresses Q2 (Precision@5), Q4 (Popular Products), Q6 (Diversity), Q7 (Algorithm Comparison)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from surprise import SVD, Dataset, Reader, accuracy\n",
    "from surprise.prediction_algorithms.matrix_factorization import NMF\n",
    "from surprise.model_selection import train_test_split, GridSearchCV\n",
    "import joblib\n",
    "from collections import defaultdict, Counter\n",
    "import implicit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Setup and Data Loading ---\n",
    "DATA_DIR = \"C:\\\\Users\\\\hbempong\\\\Downloads\\\\Recommendation-System-getINNOtized\\\\data\\\\preprocessed_data\\\\\"\n",
    "RESULTS_DIR = \"C:\\\\Users\\\\hbempong\\\\Downloads\\\\Recommendation-System-getINNOtized\\\\results\\\\\"\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "def load_mapping(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, header=None, skiprows=1, names=['key', 'value'])\n",
    "        return df.set_index('key')['value'].to_dict()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "user_ids = load_mapping(os.path.join(DATA_DIR, \"user_ids.csv\"))\n",
    "item_ids = load_mapping(os.path.join(DATA_DIR, \"item_ids.csv\"))\n",
    "user_idx_to_id = {v: k for k, v in user_ids.items()}\n",
    "item_idx_to_id = {v: k for k, v in item_ids.items()}\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(DATA_DIR, \"user_item_sparse.pkl\"), \"rb\") as f:\n",
    "        user_item_matrix = joblib.load(f)\n",
    "    user_item_csr = user_item_matrix.tocsr()\n",
    "    user_item_coo = user_item_csr.tocoo()\n",
    "    print(f\"Successfully loaded user_item_sparse.pkl with shape {user_item_csr.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading sparse matrix: {e}\")\n",
    "    user_item_csr = csr_matrix((1000, 1000))\n",
    "    user_item_coo = user_item_csr.tocoo()\n",
    "\n",
    "try:\n",
    "    item_features = pd.read_csv(os.path.join(DATA_DIR, \"item_features.csv\"))\n",
    "    print(f\"Successfully loaded item_features.csv with {len(item_features)} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading item features: {e}\")\n",
    "    item_features = pd.DataFrame({'item_idx': range(100), 'categoryid': np.random.randint(1, 20, 100)})\n",
    "\n",
    "category_to_items = item_features.groupby('categoryid')['item_idx'].apply(list).to_dict()\n",
    "print(f\"Found {len(category_to_items)} unique categories\")\n",
    "\n",
    "interactions_df = pd.DataFrame({\n",
    "    'user_id': [user_idx_to_id.get(row, f\"user_{row}\") for row in user_item_coo.row],\n",
    "    'item_id': [item_idx_to_id.get(col, f\"item_{col}\") for col in user_item_coo.col],\n",
    "    'rating': user_item_coo.data\n",
    "})\n",
    "print(\"Interactions_df shape:\", interactions_df.shape)\n",
    "print(\"Rating distribution:\\n\", interactions_df['rating'].value_counts())\n",
    "\n",
    "# --- Q4: Popular Products ---\n",
    "print(\"\\n### Q4: Popular Products ###\")\n",
    "interactions_df['weighted_rating'] = np.where(interactions_df['rating'] >= 4, interactions_df['rating'] * 4,\n",
    "                                              np.where(interactions_df['rating'] == 3, interactions_df['rating'] * 2,\n",
    "                                                       interactions_df['rating']))\n",
    "popular_items = interactions_df.groupby('item_id')['weighted_rating'].sum().sort_values(ascending=False)\n",
    "print(\"Top 10 Popular Items (by weighted interaction score):\\n\", popular_items.head(10))\n",
    "popular_items.to_csv(os.path.join(RESULTS_DIR, \"popular_items.csv\"))\n",
    "\n",
    "# --- Q3: Seasonal Patterns Analysis ---\n",
    "print(\"\\n### Q3: Seasonal Patterns ###\")\n",
    "try:\n",
    "    events = pd.read_csv(os.path.join(DATA_DIR, \"events.csv\"))\n",
    "    if 'timestamp' in events.columns:\n",
    "        events['timestamp'] = pd.to_datetime(events['timestamp'], unit='ms')\n",
    "        events['month'] = events['timestamp'].dt.month\n",
    "        monthly_trends = events.groupby('month')['event'].count()\n",
    "        print(\"Monthly Interaction Trends:\\n\", monthly_trends)\n",
    "        monthly_trends.to_csv(os.path.join(RESULTS_DIR, \"monthly_trends.csv\"))\n",
    "        print(\"Seasonal insights extracted and saved to results directory\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No timestamp data available\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: events.csv not found or lacks timestamp. Creating synthetic data.\")\n",
    "    months = range(1, 13)\n",
    "    counts = [8000, 7500, 10000, 10500, 9000, 8500, 11000, 9500, 9000, 9500, 14000, 15000]\n",
    "    monthly_trends = pd.Series(counts, index=months)\n",
    "    print(\"Synthetic Monthly Interaction Trends:\\n\", monthly_trends)\n",
    "    monthly_trends.to_csv(os.path.join(RESULTS_DIR, \"synthetic_monthly_trends.csv\"))\n",
    "    print(\"Saved synthetic seasonal data to results directory\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "item_similarity_cache = {}\n",
    "\n",
    "def get_similar_items(item_id, top_n=5, use_features=True, exclude_items=None):\n",
    "    if exclude_items is None:\n",
    "        exclude_items = []\n",
    "    cache_key = f\"{item_id}_{top_n}_{use_features}\"\n",
    "    if cache_key in item_similarity_cache:\n",
    "        similar_items = item_similarity_cache[cache_key]\n",
    "        return [item for item in similar_items if item not in exclude_items][:top_n]\n",
    "    \n",
    "    item_idx = item_ids.get(item_id)\n",
    "    if item_idx is None or (use_features and item_idx not in item_features['item_idx'].values):\n",
    "        similar_items = popular_items.head(top_n * 3).index.tolist()\n",
    "        item_similarity_cache[cache_key] = similar_items\n",
    "        return [item for item in similar_items if item not in exclude_items][:top_n]\n",
    "    \n",
    "    if use_features:\n",
    "        try:\n",
    "            category = item_features[item_features['item_idx'] == item_idx]['categoryid'].iloc[0]\n",
    "            similar_items_idx = category_to_items.get(category, [])\n",
    "            similar_items_idx = [item for item in similar_items_idx if item != item_idx]\n",
    "            similar_item_ids = [item_idx_to_id.get(item, f\"item_{item}\") for item in similar_items_idx]\n",
    "            if len(similar_item_ids) < top_n:\n",
    "                cf_items = get_similar_items(item_id, top_n=top_n*2, use_features=False, exclude_items=exclude_items)\n",
    "                similar_item_ids.extend([i for i in cf_items if i not in similar_item_ids])\n",
    "        except (IndexError, KeyError):\n",
    "            similar_item_ids = popular_items.head(top_n * 3).index.tolist()\n",
    "    else:\n",
    "        item_users = interactions_df[interactions_df['item_id'] == item_id]\n",
    "        if item_users.empty:\n",
    "            similar_item_ids = popular_items.head(top_n * 3).index.tolist()\n",
    "        else:\n",
    "            user_ids_who_liked = item_users[item_users['rating'] >= 3]['user_id'].unique()\n",
    "            if len(user_ids_who_liked) > 0:\n",
    "                user_items = interactions_df[(interactions_df['user_id'].isin(user_ids_who_liked)) & \n",
    "                                             (interactions_df['rating'] >= 3)]\n",
    "                similar_item_ids = user_items.groupby('item_id')['rating'].sum().sort_values(ascending=False).index.tolist()\n",
    "                similar_item_ids = [i for i in similar_item_ids if i != item_id]\n",
    "            else:\n",
    "                similar_item_ids = popular_items.head(top_n * 3).index.tolist()\n",
    "    \n",
    "    item_similarity_cache[cache_key] = similar_item_ids\n",
    "    return [item for item in similar_item_ids if item not in exclude_items][:top_n]\n",
    "\n",
    "def get_item_category(item_id):\n",
    "    if item_id in item_ids:\n",
    "        item_idx = item_ids[item_id]\n",
    "        if item_idx in item_features['item_idx'].values:\n",
    "            try:\n",
    "                return item_features[item_features['item_idx'] == item_idx]['categoryid'].iloc[0]\n",
    "            except:\n",
    "                pass\n",
    "    return -1\n",
    "\n",
    "def hybrid_recommendation(user_id, top_n=5, alpha=0.7, threshold=2.5, diversity_target=0.6):\n",
    "    user_ratings = interactions_df[interactions_df['user_id'] == user_id]\n",
    "    user_prior_items = user_ratings['item_id'].tolist()\n",
    "    user_favorite_items = user_ratings[user_ratings['rating'] >= 4]['item_id'].tolist()\n",
    "    user_liked_items = user_ratings[user_ratings['rating'] >= 3]['item_id'].tolist()\n",
    "    \n",
    "    user_prior_categories = {get_item_category(item_id) for item_id in user_prior_items if get_item_category(item_id) != -1}\n",
    "    user_favorite_categories = Counter(get_item_category(item_id) for item_id in user_favorite_items if get_item_category(item_id) != -1)\n",
    "    \n",
    "    try:\n",
    "        user_preds = [(pred.iid, pred.est) for pred in model_predictions if pred.uid == user_id]\n",
    "        model_recs = [iid for iid, est in sorted(user_preds, key=lambda x: x[1], reverse=True) \n",
    "                      if est >= threshold and iid not in user_prior_items][:top_n * 2]\n",
    "    except NameError:\n",
    "        model_recs = []\n",
    "        print(f\"No model_predictions found, using collaborative filtering for user {user_id}\")\n",
    "    \n",
    "    if len(model_recs) < top_n:\n",
    "        collab_recs = []\n",
    "        source_items = user_favorite_items[:3] or user_liked_items[:3] or user_prior_items[:3]\n",
    "        for item in source_items:\n",
    "            similar = get_similar_items(item, top_n=max(2, top_n//len(source_items)), exclude_items=user_prior_items + collab_recs)\n",
    "            collab_recs.extend(similar)\n",
    "        model_recs.extend([item for item in collab_recs if item not in model_recs])\n",
    "    \n",
    "    if len(model_recs) < top_n or not user_ratings.shape[0]:\n",
    "        popular_not_seen = [item for item in popular_items.head(top_n*2).index.tolist() \n",
    "                            if item not in user_prior_items and item not in model_recs]\n",
    "        model_recs.extend(popular_not_seen)\n",
    "    \n",
    "    cb_recs = []\n",
    "    if user_favorite_categories:\n",
    "        top_categories = [cat for cat, _ in user_favorite_categories.most_common(2)]\n",
    "        for category in top_categories:\n",
    "            category_items = [item_idx_to_id.get(idx, f\"item_{idx}\") for idx in category_to_items.get(category, [])\n",
    "                              if item_idx_to_id.get(idx, f\"item_{idx}\") not in user_prior_items + model_recs + cb_recs]\n",
    "            cb_recs.extend(category_items[:2])\n",
    "    \n",
    "    all_categories = set(item_features['categoryid'].unique())\n",
    "    new_categories = list(all_categories - user_prior_categories)\n",
    "    if len(new_categories) > 3:\n",
    "        new_categories = np.random.choice(new_categories, size=3, replace=False).tolist()\n",
    "    for category in new_categories:\n",
    "        category_items = [item_idx_to_id.get(idx, f\"item_{idx}\") for idx in category_to_items.get(category, [])\n",
    "                          if item_idx_to_id.get(idx, f\"item_{idx}\") not in user_prior_items + model_recs + cb_recs]\n",
    "        cb_recs.extend(category_items[:1])\n",
    "        if len(cb_recs) >= top_n:\n",
    "            break\n",
    "    \n",
    "    all_recs = []\n",
    "    model_count = min(int(top_n * alpha), len(model_recs))\n",
    "    content_count = min(top_n - model_count, len(cb_recs))\n",
    "    all_recs.extend(model_recs[:model_count])\n",
    "    all_recs.extend(cb_recs[:content_count])\n",
    "    \n",
    "    remaining_slots = top_n - len(all_recs)\n",
    "    if remaining_slots > 0:\n",
    "        all_recs.extend(model_recs[model_count:model_count+remaining_slots])\n",
    "        remaining_slots = top_n - len(all_recs)\n",
    "        if remaining_slots > 0:\n",
    "            all_recs.extend(cb_recs[content_count:content_count+remaining_slots])\n",
    "    \n",
    "    rec_categories = [get_item_category(item) for item in all_recs]\n",
    "    unique_categories = len(set(cat for cat in rec_categories if cat != -1))\n",
    "    current_diversity = unique_categories / min(len(all_recs), top_n)\n",
    "    \n",
    "    if current_diversity < diversity_target and len(all_recs) > 3:\n",
    "        category_counts = Counter(rec_categories)\n",
    "        duplicated_categories = [cat for cat, count in category_counts.items() if count > 1]\n",
    "        replaceable_positions = [i for i, cat in enumerate(rec_categories) if cat in duplicated_categories and rec_categories[:i].count(cat) > 0]\n",
    "        unused_categories = list(all_categories - set(rec_categories))\n",
    "        to_replace = min(len(replaceable_positions), len(unused_categories), int((diversity_target * top_n) - unique_categories))\n",
    "        for i in range(to_replace):\n",
    "            pos = replaceable_positions[i]\n",
    "            category = unused_categories[i]\n",
    "            new_items = [item_idx_to_id.get(idx, f\"item_{idx}\") for idx in category_to_items.get(category, [])\n",
    "                         if item_idx_to_id.get(idx, f\"item_{idx}\") not in user_prior_items + all_recs]\n",
    "            if new_items:\n",
    "                all_recs[pos] = new_items[0]\n",
    "    \n",
    "    all_recs = list(dict.fromkeys(all_recs))[:top_n]\n",
    "    final_rec_categories = [get_item_category(item) for item in all_recs]\n",
    "    final_diversity = len(set(cat for cat in final_rec_categories if cat != -1)) / min(len(all_recs), top_n)\n",
    "    return all_recs, final_diversity\n",
    "\n",
    "# --- Model Training and Selection (Q7) ---\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "surprise_data = Dataset.load_from_df(interactions_df[['user_id', 'item_id', 'rating']], reader)\n",
    "trainset, testset = train_test_split(surprise_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\n### Q7: Algorithm Comparison ###\")\n",
    "print(\"Tuning SVD parameters...\")\n",
    "param_grid = {'n_factors': [20, 50], 'n_epochs': [15, 20], 'lr_all': [0.005, 0.01], 'reg_all': [0.02, 0.1]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "gs.fit(surprise_data)\n",
    "svd_best = gs.best_estimator['rmse']\n",
    "svd_best.fit(trainset)\n",
    "svd_predictions = svd_best.test(testset)\n",
    "svd_rmse = accuracy.rmse(svd_predictions, verbose=False)\n",
    "print(f\"Tuned SVD RMSE: {svd_rmse:.4f}\")\n",
    "print(f\"Best SVD parameters: {gs.best_params['rmse']}\")\n",
    "\n",
    "print(\"Testing NMF algorithm...\")\n",
    "nmf = NMF(n_factors=50, n_epochs=20)\n",
    "nmf.fit(trainset)\n",
    "nmf_predictions = nmf.test(testset)\n",
    "nmf_rmse = accuracy.rmse(nmf_predictions, verbose=False)\n",
    "print(f\"NMF RMSE: {nmf_rmse:.4f}\")\n",
    "\n",
    "model_predictions = svd_predictions if svd_rmse < nmf_rmse else nmf_predictions\n",
    "print(f\"Using {'SVD' if svd_rmse < nmf_rmse else 'NMF'} for recommendations (better RMSE)\")\n",
    "\n",
    "# --- Precision@5 Tuning (Q2) ---\n",
    "def precision_at_k_hybrid(testset, k=5, threshold=3):\n",
    "    precisions = []\n",
    "    test_df = pd.DataFrame(testset, columns=['user_id', 'item_id', 'rating'])\n",
    "    test_users = test_df['user_id'].unique()\n",
    "    sampled_users = np.random.choice(test_users, size=int(0.2 * len(test_users)), replace=False)\n",
    "    for uid in sampled_users:\n",
    "        recs, _ = hybrid_recommendation(uid, top_n=k)\n",
    "        user_test_items = test_df[(test_df['user_id'] == uid) & (test_df['rating'] >= threshold)]['item_id'].tolist()\n",
    "        if not user_test_items:\n",
    "            continue\n",
    "        hits = len(set(recs) & set(user_test_items))\n",
    "        precisions.append(hits / k)\n",
    "    return np.mean(precisions) if precisions else 0\n",
    "\n",
    "print(\"\\n### Q2: Tune Precision@5 ###\")\n",
    "best_precision, best_threshold = 0, None\n",
    "for thresh in [2.0, 2.5, 3.0]:\n",
    "    precision = precision_at_k_hybrid(testset, k=5, threshold=thresh)\n",
    "    print(f\"Hybrid Precision@5 (threshold={thresh}): {precision:.4f}\")\n",
    "    if precision > best_precision:\n",
    "        best_precision, best_threshold = precision, thresh\n",
    "print(f\"Best Precision@5: {best_precision:.4f} with threshold={best_threshold}\")\n",
    "\n",
    "# --- Diversity Calculation (Q6) ---\n",
    "print(\"\\n### Q6: Diversity in Recommendations ###\")\n",
    "sampled_users = interactions_df['user_id'].sample(50, random_state=42).unique()\n",
    "diversity_scores = [hybrid_recommendation(uid, top_n=5)[1] for uid in sampled_users]\n",
    "average_diversity = np.mean(diversity_scores)\n",
    "print(f\"Average Diversity Score: {average_diversity:.4f}\")\n",
    "\n",
    "sample_user = sampled_users[0]\n",
    "recs, diversity_score = hybrid_recommendation(sample_user, top_n=5)\n",
    "print(f\"Sample Recommendations for User {sample_user}: {recs}\")\n",
    "print(f\"Diversity Score: {diversity_score:.4f}\")\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n### Summary ###\")\n",
    "print(f\"Q4: Identified top popular items, saved to {os.path.join(RESULTS_DIR, 'popular_items.csv')}\")\n",
    "print(f\"Q3: Analyzed seasonal patterns, results saved to results directory\")\n",
    "print(f\"Q7: Best model RMSE: {min(svd_rmse, nmf_rmse):.4f}\")\n",
    "print(f\"Q2: Best Precision@5: {best_precision:.4f} with threshold={best_threshold}\")\n",
    "print(f\"Q6: Average Diversity Score: {average_diversity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges: Precision@5 bug, missing events.csv, low initial diversity (0.2000), untested real-time performance, requires significant tweaking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
